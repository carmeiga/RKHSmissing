Z <- rMvdc(n, myMvd)
Z= Z[,1]
eps=rnorm(n,0,1)
Y= 2*Z+eps
#Y=2*(Z[,1])+eps
#colnames(Z) <- c("x1", "x2", "x3")
#pairs.panels(Z)
beta=c(-0.05,0,0)
beta= beta[1]
p=numeric(n) #vector with pi
for(i in 1:n) {
#x=Z[i,]
x= Z[i]
p[i]=inv.logit(sum(x*beta), min = 0, max = 1)
}
M=numeric(n)
for(i in 1:n) {
M[i]=rbinom(1,1,1-p[i])
}
Yorig=Y
Y[!M]=NA
#setwd("~/Descargas/RKHSmissing-master/superlearner")
source("incompletenew.R")
res=incomplete.reg(Z,Y)
plot(p~as.numeric(res$p))
plot(Y[!is.na(Y)],res$Ypred[!is.na(Y)])
plot(Yorig,res$Ypred)
####################################################33
#setwd("~/Descargas/RKHSmissing-master/kernel_machine")
library("Rcpp")
library(RcppArmadillo)
sourceCpp("regression.cpp")
Y[is.na(Y)]=0
mu=res$Ypred
lambda=0.05
d=as.numeric(M/p)
Ytilde=Y[res$p>0.1]
#Ztilde=Z[res$p>0.1,]
Ztilde=Z[res$p>0.1]
mutilde=mu[res$p>0.1]
dtilde=d[res$p>0.1]
#d=d/sum(d)
Wtilde=(diag(dtilde))
W=diag(d)
#alphatilde=coef(Ztilde,Ytilde,Wtilde,lambda,mutilde,3)
Z= as.matrix(Z)
distanciasZ= dist(Z)
median(distanciasZ^2)
waux= M/res$p
#waux= M/p
w= diag(as.numeric(waux),n,n)
# borre W
alphatilde=coef(Z,Y,w,0.5,mu,sqrt(1/8.5))
prediction=numeric(n);
for(i in 1:n) {
prediction[i]=kernel_machine(Z,as.numeric(Z[i,]),as.numeric(alphatilde),sqrt(1/8.5))
}
plot(prediction, Yorig)
#plot(alphatilde,new)
# library("listdtr")
# kernelridgestandart=krr(Z, Y, group = NULL)
library("SuperLearner")
library(copula)
library(psych)
library(gtools)
library("ranger")
library("glmnet")
library("gam")
library("kernlab")
library(WeightIt)
n=100
myCop <- normalCopula(param=c(0.3,0.7,0.8), dim = 3, dispstr = "un")
myMvd <- mvdc(copula=myCop, margins=c("unif", "unif", "unif"),
paramMargins=list(list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10)) )
Z <- rMvdc(n, myMvd)
Z= Z[,1]
eps=rnorm(n,0,1)
Y= 2*Z+eps
#Y=2*(Z[,1])+eps
#colnames(Z) <- c("x1", "x2", "x3")
#pairs.panels(Z)
beta=c(-0.05,0,0)
beta= beta[1]
p=numeric(n) #vector with pi
for(i in 1:n) {
#x=Z[i,]
x= Z[i]
p[i]=inv.logit(sum(x*beta), min = 0, max = 1)
}
M=numeric(n)
for(i in 1:n) {
M[i]=rbinom(1,1,1-p[i])
}
Yorig=Y
Y[!M]=NA
#setwd("~/Descargas/RKHSmissing-master/superlearner")
source("incompletenew.R")
res=incomplete.reg(Z,Y)
plot(p~as.numeric(res$p))
plot(Y[!is.na(Y)],res$Ypred[!is.na(Y)])
plot(Yorig,res$Ypred)
####################################################33
#setwd("~/Descargas/RKHSmissing-master/kernel_machine")
library("Rcpp")
library(RcppArmadillo)
sourceCpp("regression.cpp")
Y[is.na(Y)]=0
mu=res$Ypred
lambda=0.05
d=as.numeric(M/p)
Ytilde=Y[res$p>0.1]
#Ztilde=Z[res$p>0.1,]
Ztilde=Z[res$p>0.1]
mutilde=mu[res$p>0.1]
dtilde=d[res$p>0.1]
#d=d/sum(d)
Wtilde=(diag(dtilde))
W=diag(d)
#alphatilde=coef(Ztilde,Ytilde,Wtilde,lambda,mutilde,3)
Z= as.matrix(Z)
distanciasZ= dist(Z)
median(distanciasZ^2)
waux= M/res$p
#waux= M/p
w= diag(as.numeric(waux),n,n)
# borre W
alphatilde=coef(Z,Y,w,0.5,mu,sqrt(1/8.5))
prediction=numeric(n);
for(i in 1:n) {
prediction[i]=kernel_machine(Z,as.numeric(Z[i,]),as.numeric(alphatilde),sqrt(1/8.5))
}
plot(prediction, Yorig)
#plot(alphatilde,new)
# library("listdtr")
# kernelridgestandart=krr(Z, Y, group = NULL)
sourceCpp("../kernel_machine/regression.cpp")
library("SuperLearner")
library(copula)
library(psych)
library(gtools)
library("ranger")
library("glmnet")
library("gam")
library("kernlab")
library(WeightIt)
n=100
myCop <- normalCopula(param=c(0.3,0.7,0.8), dim = 3, dispstr = "un")
myMvd <- mvdc(copula=myCop, margins=c("unif", "unif", "unif"),
paramMargins=list(list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10)) )
Z <- rMvdc(n, myMvd)
Z= Z[,1]
eps=rnorm(n,0,1)
Y= 2*Z+eps
#Y=2*(Z[,1])+eps
#colnames(Z) <- c("x1", "x2", "x3")
#pairs.panels(Z)
beta=c(-0.05,0,0)
beta= beta[1]
p=numeric(n) #vector with pi
for(i in 1:n) {
#x=Z[i,]
x= Z[i]
p[i]=inv.logit(sum(x*beta), min = 0, max = 1)
}
M=numeric(n)
for(i in 1:n) {
M[i]=rbinom(1,1,1-p[i])
}
Yorig=Y
Y[!M]=NA
#setwd("~/Descargas/RKHSmissing-master/superlearner")
source("incompletenew.R")
res=incomplete.reg(Z,Y)
plot(p~as.numeric(res$p))
plot(Y[!is.na(Y)],res$Ypred[!is.na(Y)])
plot(Yorig,res$Ypred)
####################################################33
#setwd("~/Descargas/RKHSmissing-master/kernel_machine")
library("Rcpp")
library(RcppArmadillo)
sourceCpp("../kernel_machine/regression.cpp")
Y[is.na(Y)]=0
mu=res$Ypred
lambda=0.05
d=as.numeric(M/p)
Ytilde=Y[res$p>0.1]
#Ztilde=Z[res$p>0.1,]
Ztilde=Z[res$p>0.1]
mutilde=mu[res$p>0.1]
dtilde=d[res$p>0.1]
#d=d/sum(d)
Wtilde=(diag(dtilde))
W=diag(d)
#alphatilde=coef(Ztilde,Ytilde,Wtilde,lambda,mutilde,3)
Z= as.matrix(Z)
distanciasZ= dist(Z)
median(distanciasZ^2)
waux= M/res$p
#waux= M/p
w= diag(as.numeric(waux),n,n)
# borre W
alphatilde=coef(Z,Y,w,0.5,mu,sqrt(1/8.5))
prediction=numeric(n);
for(i in 1:n) {
prediction[i]=kernel_machine(Z,as.numeric(Z[i,]),as.numeric(alphatilde),sqrt(1/8.5))
}
plot(prediction, Yorig)
#plot(alphatilde,new)
# library("listdtr")
# kernelridgestandart=krr(Z, Y, group = NULL)
?normalCopula
n=400
myCop <- normalCopula(param=0.4, dim = 5, dispstr = "un")
myMvd <- mvdc(copula=myCop, margins=c("unif", "unif", "unif"),
paramMargins=list(list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10)))
Z <- rMvdc(n, myMvd)
Z= Z[,1]
eps=rnorm(n,0,1)
Y= 2*Z+eps
n=400
myCop <- normalCopula(param=0.4, dim = 5, dispstr = "un")
myMvd <- mvdc(copula=myCop, margins=c("unif", "unif", "unif","unif","unif"),
paramMargins=list(list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10)))
Z <- rMvdc(n, myMvd)
n=400
myCop <- normalCopula(param=0.4, dim = 5, dispstr = "un")
n=400
myCop <- normalCopula(0.4, dim = 5, dispstr = "un")
myMvd <- mvdc(copula=myCop, margins=c("unif", "unif", "unif","unif","unif"),
paramMargins=list(list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10)))
n=400
p=5
myCop <- normalCopula(rep(0.4,p * (p - 1) / 2), dim = 5, dispstr = "un")
myMvd <- mvdc(copula=myCop, margins=c("unif", "unif", "unif","unif","unif"),
paramMargins=list(list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10)))
Z <- rMvdc(n, myMvd)
Z= Z[,1]
eps=rnorm(n,0,1)
Y= 2*Z+eps
airs.panels(Z)
pairs.panels(Z)
Z <- rMvdc(n, myMvd)
#Z= Z[,1]
eps=rnorm(n,0,1)
Y= 2*Z[,1]+1.5*Z[,2] + 1*Z[,3] + 0.5*Z[,4]+eps
pairs.panels(Z)
Y= 2*Z[,1]+1.5*Z[,2] + 1*Z[,3] + 0.5*Z[,4]+eps
beta=c(-0.05,0,0,0,0)
#beta= beta[1]
p=numeric(n) #vector with pi
for(i in 1:n) {
x=Z[i,]
#x= Z[i]
p[i]=inv.logit(sum(x*beta), min = 0, max = 1)
}
M=numeric(n)
for(i in 1:n) {
M[i]=rbinom(1,1,1-p[i])
}
Yorig=Y
Y[!M]=NA
Y
source("incompletenew.R")
res=incomplete.reg(Z,Y)
plot(p~as.numeric(res$p))
plot(Y[!is.na(Y)],res$Ypred[!is.na(Y)])
plot(Yorig,res$Ypred)
res$Ypred
length(Yorig)
length(res$Ypred)
n=400
p=5
myCop <- normalCopula(rep(0.4,p * (p - 1) / 2), dim = 5, dispstr = "un")
myMvd <- mvdc(copula=myCop, margins=c("unif", "unif", "unif","unif","unif"),
paramMargins=list(list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10)))
Z <- rMvdc(n, myMvd)
#Z= Z[,1]
eps=rnorm(n,0,1)
Y= 2*Z[,1]+1.5*Z[,2] + 1*Z[,3] + 0.5*Z[,4]+eps
#Y=2*(Z[,1])+eps
#colnames(Z) <- c("x1", "x2", "x3")
#pairs.panels(Z)
beta=c(-0.05,0,0,0,0)
#beta= beta[1]
p=numeric(n) #vector with pi
for(i in 1:n) {
x=Z[i,]
#x= Z[i]
p[i]=inv.logit(sum(x*beta), min = 0, max = 1)
}
M=numeric(n)
for(i in 1:n) {
M[i]=rbinom(1,1,1-p[i])
}
Yorig=Y
Y[!M]=NA
#setwd("~/Descargas/RKHSmissing-master/superlearner")
source("incompletenew.R")
res=incomplete.reg(Z,Y)
plot(p~as.numeric(res$p))
plot(Y[!is.na(Y)],res$Ypred[!is.na(Y)])
plot(Yorig,res$Ypred)
plot(Yorig,res$Ypred)
library("SuperLearner")
library(copula)
library(psych)
library(gtools)
library("ranger")
library("glmnet")
library("gam")
library("kernlab")
incomplete.reg=function(X,Y) {
n=length(Y)
M=as.numeric(is.na(Y))
X= as.data.frame(X)
pX= dim(X)[2]
#   if(pX==1){
#     repetir= rep(1,n)
#     X= data.frame(X,repetir)
#     pX=2
#   }
#
# X= as.data.frame(X)
# Añadi GLM
slmis = SuperLearner(Y = M, X = X, family = binomial(),SL.library = c("SL.mean","SL.glm", "SL.ranger", "SL.gam", "SL.ksvm"))
# porque tiñas Z
pred = predict(slmis, as.data.frame(X), onlySL = TRUE)
p=pred[[1]]
Ycomplete=Y[!is.na(Y)]
Xnew= X[!is.na(Y),]
Xnew= as.data.frame(Xnew)
colnames(Xnew)= colnames(X)
if(pX>1){
sl = SuperLearner(Y = Ycomplete, X = as.data.frame(Xnew), family = gaussian(),SL.library = c("SL.mean","SL.lm", "SL.ranger", "SL.gam", "SL.ksvm"))
pred2 = predict(sl, newdata=as.data.frame(X))
Ypred=pred2[[1]]
}else{
sl = SuperLearner(Y = Ycomplete, X = Xnew, family = gaussian(),SL.library = c("SL.mean","SL.ranger", "SL.lm","SL.gam", "SL.ksvm"))
pred2 = predict(sl, newdata=as.data.frame(X))
Ypred=pred2[[1]]
}
return(list("Ypred" = Ypred , "p" = p))
}
res=incomplete.reg(Z,Y)
plot(p~as.numeric(res$p))
plot(Y[!is.na(Y)],res$Ypred[!is.na(Y)])
plot(Yorig,res$Ypred)
plot(p~as.numeric(res$p))
n=2000
p=5
myCop <- normalCopula(rep(0.4,p * (p - 1) / 2), dim = 5, dispstr = "un")
myMvd <- mvdc(copula=myCop, margins=c("unif", "unif", "unif","unif","unif"),
paramMargins=list(list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10)))
Z <- rMvdc(n, myMvd)
#Z= Z[,1]
eps=rnorm(n,0,1)
Y= 2*Z[,1]+1.5*Z[,2] + 1*Z[,3] + 0.5*Z[,4]+eps
#Y=2*(Z[,1])+eps
#colnames(Z) <- c("x1", "x2", "x3")
#pairs.panels(Z)
beta=c(-0.05,0,0,0,0)
#beta= beta[1]
p=numeric(n) #vector with pi
for(i in 1:n) {
x=Z[i,]
#x= Z[i]
p[i]=inv.logit(sum(x*beta), min = 0, max = 1)
}
M=numeric(n)
for(i in 1:n) {
M[i]=rbinom(1,1,1-p[i])
}
Yorig=Y
Y[!M]=NA
#setwd("~/Descargas/RKHSmissing-master/superlearner")
source("incompletenew.R")
res=incomplete.reg(Z,Y)
plot(p~as.numeric(res$p))
plot(Yorig,res$Ypred)
plot(p~as.numeric(res$p))
points(p[M==1]~as.numeric(res$p[M==1]),col='red')
library("Rcpp")
library(RcppArmadillo)
sourceCpp("../kernel_machine/regression.cpp")
Y[is.na(Y)]=0
mu=res$Ypred
lambda=0.05
d=as.numeric(M/p)
Ytilde=Y[res$p>0.1]
#Ztilde=Z[res$p>0.1,]
Ztilde=Z[res$p>0.1]
mutilde=mu[res$p>0.1]
dtilde=d[res$p>0.1]
#d=d/sum(d)
Wtilde=(diag(dtilde))
W=diag(d)
#alphatilde=coef(Ztilde,Ytilde,Wtilde,lambda,mutilde,3)
Z= as.matrix(Z)
distanciasZ= dist(Z)
median(distanciasZ^2)
distanciasZ= dist(Z)
median(distanciasZ^2)
waux= M/res$p
#waux= M/p
w= diag(as.numeric(waux),n,n)
# borre W
alphatilde=coef(Z,Y,w,0.5,mu,sqrt(1/70.26))
prediction=numeric(n);
for(i in 1:n) {
prediction[i]=kernel_machine(Z,as.numeric(Z[i,]),as.numeric(alphatilde),sqrt(1/70.26))
}
plot(prediction, Yorig)
Rcpp::sourceCpp('~/repodir/RKHSmissing/marcos/RKHSmissing-master/kernel_machine/regression.cpp')
alphatilde=coef(Z,Y,w,0.5,mu,sqrt(1/70.26))
prediction=numeric(n);
for(i in 1:n) {
prediction[i]=kernel_machine(Z,as.numeric(Z[i,]),as.numeric(alphatilde),sqrt(1/70.26))
}
plot(prediction, Yorig)
alphatilde
prediction=numeric(n);
for(i in 1:n) {
prediction[i]=kernel_machine(Z,as.numeric(Z[i,]),as.numeric(alphatilde),sqrt(1/70.26))
}
plot(prediction, Yorig)
K=Kernelfull(Z,sqrt(1/70.26))
Kernle
pred2=K%*%alphatilde
plot(pred2,Yorig)
plot(pred2,Yorig)
points(pred2[M==1],Yorig[M==1],col='red')
Rcpp::sourceCpp('~/repodir/RKHSmissing/marcos/RKHSmissing-master/kernel_machine/regression.cpp')
Rcpp::sourceCpp('~/repodir/RKHSmissing/marcos/RKHSmissing-master/kernel_machine/regression.cpp')
waux= M/res$p
#waux= M/p
w= diag(as.numeric(waux),n,n)
# borre W
alphatilde=coef(Z,Y,w,0.5,mu,sqrt(1/70.26))
prediction=numeric(n);
K=Kernelfull(Z,sqrt(1/70.26))
pred2=K%*%alphatilde
points(pred2[M==1],Yorig[M==1],col='red')
points(pred2[M==1],Yorig[M==1],col='red')
K=Kernelfull(Z,sqrt(1/70.26))
pred2=K%*%alphatilde
plot(pred2,Yorig)
points(pred2[M==1],Yorig[M==1],col='red')
n=2000
p=5
myCop <- normalCopula(rep(0.4,p * (p - 1) / 2), dim = 5, dispstr = "un")
myMvd <- mvdc(copula=myCop, margins=c("unif", "unif", "unif","unif","unif"),
paramMargins=list(list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10),
list(min=0,max=10)))
Z <- rMvdc(n, myMvd)
#Z= Z[,1]
eps=rnorm(n,0,1)
#Y= 2*Z[,1]+1.5*Z[,2] + 1*Z[,3] + 0.5*Z[,4]+eps
Y=2*(Z[,1])+eps
#colnames(Z) <- c("x1", "x2", "x3")
#pairs.panels(Z)
beta=c(-0.01,-0.01,-0.01,-0.01,-0.01)
#beta= beta[1]
p=numeric(n) #vector with pi
for(i in 1:n) {
x=Z[i,]
#x= Z[i]
p[i]=inv.logit(sum(x*beta), min = 0, max = 1)
}
M=numeric(n)
for(i in 1:n) {
M[i]=rbinom(1,1,1-p[i])
}
Yorig=Y
Y[!M]=NA
M
res=incomplete.reg(Z,Y)
plot(p~as.numeric(res$p))
points(p[M==1]~as.numeric(res$p[M==1]),col='red')
plot(Y[!is.na(Y)],res$Ypred[!is.na(Y)])
plot(Yorig,res$Ypred)
plot(p~as.numeric(res$p))
